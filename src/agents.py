"""
Defini√ß√µes dos agentes do sistema VerbaFlow.
Suporta Groq (prim√°rio) e Gemini (fallback).
"""
import os
from typing import Optional
from langchain_groq import ChatGroq
from crewai import Agent
from crewai.llm import LLM
from src.tools import get_tavily_tool
from src.config import get_config

# Import opcional do Gemini (pode n√£o estar instalado)
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Import LiteLLM para verificar disponibilidade
try:
    import litellm
    LITELLM_AVAILABLE = True
except ImportError:
    LITELLM_AVAILABLE = False


def get_llm(model_name: Optional[str] = None, provider: str = "groq"):
    """
    Configura e retorna o LLM com suporte a m√∫ltiplos provedores.
    
    Args:
        model_name: Nome do modelo a usar. Se None, usa o padr√£o do provider.
        provider: "groq" (padr√£o) ou "gemini" (fallback)
    
    Returns:
        LLM configurado (CrewAI LLM para Groq, LangChain LLM para Gemini)
    
    Raises:
        ValueError: Se as credenciais necess√°rias n√£o estiverem dispon√≠veis
    """
    config = get_config()
    
    if provider == "groq":
        api_key = config.groq_api_key or os.getenv("GROQ_API_KEY")
        if not api_key:
            # Verificar se h√° chave do Gemini dispon√≠vel para fallback
            gemini_key = (
                config.google_api_key or 
                os.getenv("GOOGLE_API_KEY") or 
                os.getenv("GEMINI_API_KEY")
            )
            if config.use_gemini_fallback and gemini_key:
                # Fallback autom√°tico para Gemini
                return get_llm(model_name=config.gemini_model, provider="gemini")
            raise ValueError("GROQ_API_KEY n√£o encontrada e fallback Gemini n√£o configurado")
        
        model = model_name or config.groq_model
        
        return LLM(
            model=model,
            api_key=api_key,
            base_url="https://api.groq.com/openai/v1",
            temperature=config.temperature
        )
    
    elif provider == "gemini":
        if not GEMINI_AVAILABLE:
            raise ValueError("Gemini n√£o est√° dispon√≠vel. Instale: pip install langchain-google-genai")
        
        # Aceitar tanto GOOGLE_API_KEY quanto GEMINI_API_KEY
        api_key = config.google_api_key or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY ou GEMINI_API_KEY n√£o encontrada nas vari√°veis de ambiente")
        
        model = model_name or config.gemini_model
        
        # Normalizar o nome do modelo para ChatGoogleGenerativeAI
        # Remover prefixos se presentes
        if model.startswith("gemini/"):
            model = model.replace("gemini/", "")
        elif model.startswith("models/"):
            model = model.replace("models/", "")
        
        # PROBLEMA: O CrewAI Agent tenta converter LangChain LLMs internamente
        # Quando detecta ChatGoogleGenerativeAI, tenta usar provider nativo que n√£o est√° instalado
        # Quando usa LiteLLM, passa formato errado: "models/gemini-1.5-pro" em vez de "gemini/gemini-1.5-pro"
        # 
        # SOLU√á√ÉO: Usar o wrapper LLM do CrewAI com formato LiteLLM correto
        # O formato "gemini/<modelo>" faz o CrewAI usar LiteLLM diretamente
        
        # Definir a chave do Google como vari√°vel de ambiente para LiteLLM
        os.environ["GEMINI_API_KEY"] = api_key
        os.environ["GOOGLE_API_KEY"] = api_key
        
        # Normalizar modelo (remover prefixos se presentes)
        clean_model = model
        if clean_model.startswith("gemini/"):
            clean_model = clean_model.replace("gemini/", "")
        elif clean_model.startswith("models/"):
            clean_model = clean_model.replace("models/", "")
        
        # Usar formato LiteLLM correto: "gemini/gemini-1.5-pro"
        gemini_model_name = f"gemini/{clean_model}"
        
        # Usar wrapper LLM do CrewAI que vai usar LiteLLM
        # O LiteLLM busca GEMINI_API_KEY automaticamente das vari√°veis de ambiente
        try:
            # Tentar criar LLM com formato LiteLLM
            gemini_llm = LLM(
                model=gemini_model_name,  # Formato LiteLLM: "gemini/gemini-1.5-pro"
                api_key=api_key,
                temperature=config.temperature
            )
            return gemini_llm
        except (ImportError, ValueError, Exception) as e:
            error_str = str(e).lower()
            # Se o provider nativo tentar ser usado, tentar instalar ou dar erro claro
            if "google-genai" in error_str or "gemini" in error_str or "native provider" in error_str:
                # Tentar importar o provider nativo para ver se est√° dispon√≠vel
                try:
                    from crewai.llms.providers.gemini.completion import GeminiCompletion
                    # Se chegou aqui, o provider nativo est√° dispon√≠vel, mas houve outro erro
                    raise ValueError(
                        f"Erro ao configurar Gemini com provider nativo: {e}\n\n"
                        f"**Tente:**\n"
                        f"1. Verifique se a chave API est√° correta\n"
                        f"2. Aguarde o reset do rate limit do Groq\n"
                        f"3. Use um modelo menor do Groq (llama-3.1-8b-instant)\n"
                    )
                except ImportError:
                    # Provider nativo n√£o est√° instalado
                    raise ValueError(
                        f"‚ùå **Provider nativo do Gemini n√£o est√° instalado**\n\n"
                        f"O CrewAI est√° tentando usar o provider nativo do Gemini, mas ele n√£o est√° instalado.\n"
                        f"O formato 'gemini/{clean_model}' deveria for√ßar o uso do LiteLLM, mas o CrewAI "
                        f"ainda est√° tentando o provider nativo primeiro.\n\n"
                        f"**üîß Solu√ß√µes:**\n\n"
                        f"**Op√ß√£o 1 (Recomendada):** Instale o provider nativo:\n"
                        f"```bash\n"
                        f"pip install 'crewai[google-genai]'\n"
                        f"```\n\n"
                        f"**Op√ß√£o 2:** Aguarde o reset do rate limit do Groq (~12 minutos)\n\n"
                        f"**Op√ß√£o 3:** Use um modelo menor do Groq que consome menos tokens:\n"
                        f"- `llama-3.1-8b-instant` (mais r√°pido, menos tokens)\n"
                        f"- `mixtral-8x7b-32768` (alternativa)\n\n"
                        f"**Erro original:** {e}"
                    )
            # Re-raise outros erros
            raise
    
    else:
        raise ValueError(f"Provider '{provider}' n√£o suportado. Use 'groq' ou 'gemini'")


def get_llm_with_fallback(model_name: Optional[str] = None) -> LLM:
    """
    Obt√©m LLM com fallback autom√°tico para Gemini se Groq falhar.
    
    Args:
        model_name: Nome do modelo (opcional)
    
    Returns:
        LLM configurado (Groq ou Gemini)
    """
    config = get_config()
    
    try:
        return get_llm(model_name=model_name, provider="groq")
    except (ValueError, Exception) as e:
        if config.use_gemini_fallback:
            try:
                return get_llm(model_name=config.gemini_model, provider="gemini")
            except Exception as gemini_error:
                raise ValueError(
                    f"Falha ao usar Groq: {e}. "
                    f"Falha ao usar Gemini (fallback): {gemini_error}"
                )
        raise


def create_analyst_agent(llm):
    """
    Cria o Agente 1: O Analista - Expert NLP Linguist & Classifier com Chain of Thought.
    
    Args:
        llm: Inst√¢ncia do LLM configurado
    
    Returns:
        Agent configurado com prompt engineering avan√ßado
    """
    return Agent(
        role="Expert NLP Linguist & Classifier",
        goal="Classificar textos com precis√£o m√°xima usando an√°lise passo-a-passo (Chain of Thought). "
             "SEMPRE siga o processo: 1) An√°lise de entidades, 2) Racioc√≠nio contextual, 3) Hip√≥tese com exclus√µes, "
             "4) Conclus√£o final. O output DEVE terminar com 'Category: <nome_da_categoria>' em uma linha separada.",
        backstory="""Voc√™ √© um linguista computacional de renome internacional com doutorado em NLP e mais de 15 anos 
        de experi√™ncia em classifica√ß√£o de textos. Voc√™ trabalhou no desenvolvimento do pr√≥prio dataset 20 Newsgroups 
        e conhece cada nuance das 20 categorias:
        
        **Computa√ß√£o:** comp.graphics, comp.os.ms-windows.misc, comp.sys.ibm.pc.hardware, 
        comp.sys.mac.hardware, comp.windows.x
        
        **Ci√™ncia:** sci.crypt, sci.electronics, sci.med, sci.space
        
        **Recrea√ß√£o:** rec.autos, rec.motorcycles, rec.sport.baseball, rec.sport.hockey
        
        **Religi√£o/Sociedade:** alt.atheism, soc.religion.christian, talk.religion.misc
        
        **Pol√≠tica:** talk.politics.guns, talk.politics.mideast, talk.politics.misc
        
        **Diversos:** misc.forsale
        
        Sua metodologia √© rigorosa: voc√™ NUNCA classifica sem primeiro analisar entidades-chave, considerar o contexto 
        hist√≥rico (anos 90), distinguir categorias similares (ex: comp.sys.ibm.pc.hardware vs comp.sys.mac.hardware), 
        e justificar sua decis√£o. Sua taxa de precis√£o √© superior a 95%.""",
        verbose=True,
        allow_delegation=False,
        llm=llm
    )


def create_researcher_agent(llm):
    """
    Cria o Agente 2: O Pesquisador - Fact-Checker & Context Enricher.
    
    Args:
        llm: Inst√¢ncia do LLM configurado
    
    Returns:
        Agent configurado
    """
    tavily_tool = get_tavily_tool()
    tools = [tavily_tool] if tavily_tool else []
    
    return Agent(
        role="Fact-Checker & Context Enricher",
        goal="Buscar informa√ß√µes atualizadas e contexto moderno sobre o t√≥pico identificado, validando e "
             "enriquecendo a classifica√ß√£o com dados recentes da web. Use Tavily para encontrar not√≠cias, "
             "tend√™ncias e informa√ß√µes relevantes que conectem o texto hist√≥rico (anos 90) ao contexto atual.",
        backstory="""Voc√™ √© um pesquisador s√™nior especializado em fact-checking e an√°lise de contexto temporal. 
        Trabalhou em organiza√ß√µes de m√≠dia de prest√≠gio e desenvolveu uma metodologia √∫nica para conectar informa√ß√µes 
        hist√≥ricas com o presente. Voc√™ entende que textos do dataset 20 Newsgroups s√£o dos anos 90, mas voc√™ busca 
        como esses t√≥picos evolu√≠ram at√© hoje. Sua miss√£o √© enriquecer a classifica√ß√£o com contexto moderno, 
        mostrando a relev√¢ncia atual do t√≥pico identificado.""",
        verbose=True,
        allow_delegation=False,
        llm=llm,
        tools=tools
    )


def create_editor_agent(llm):
    """
    Cria o Agente 3: O Editor Chefe - Executive Report Compiler.
    
    Args:
        llm: Inst√¢ncia do LLM configurado
    
    Returns:
        Agent configurado
    """
    return Agent(
        role="Executive Report Compiler",
        goal="Compilar a classifica√ß√£o t√©cnica e o contexto pesquisado em um relat√≥rio executivo elegante e "
             "profissional em Markdown, escrito em portugu√™s brasileiro (pt-BR). O relat√≥rio deve ser claro, "
             "bem estruturado e acess√≠vel tanto para t√©cnicos quanto para leigos.",
        backstory="""Voc√™ √© o Editor-Chefe de uma revista cient√≠fica de prest√≠gio, com mestrado em Comunica√ß√£o Cient√≠fica 
        e mais de 20 anos transformando an√°lises t√©cnicas complexas em documentos acess√≠veis e elegantes. Voc√™ domina 
        a arte de s√≠ntese, criando relat√≥rios que s√£o ao mesmo tempo precisos tecnicamente e compreens√≠veis para 
        audi√™ncias diversas. Seu estilo √© claro, profissional e sempre em portugu√™s brasileiro, mantendo o rigor 
        acad√™mico sem perder a clareza.""",
        verbose=True,
        allow_delegation=False,
        llm=llm
    )

